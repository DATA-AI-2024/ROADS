{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shap # 0.46.0\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import requests\n",
    "import time\n",
    "import os\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.cluster import KMeans\n",
    "import xgboost as xgb # 2.1.1\n",
    "import matplotlib.pyplot as plt # 3.9.2\n",
    "import holidays # 0.56\n",
    "from geopy.geocoders import Nominatim\n",
    "import pickle\n",
    "\n",
    "train = pd.read_csv('train_taxi_tims.csv', encoding='utf-8-sig', engine='python')\n",
    "test = pd.read_csv('test_taxi_tims.csv', encoding='utf-8-sig', engine='python')\n",
    "\n",
    "train = train[(train['x_axis'] != 0) & (train['y_axis'] != 0)]\n",
    "test = test[(test['x_axis'] != 0) & (test['y_axis'] != 0)]\n",
    "\n",
    "train['datetime'] = pd.to_datetime(train['datetime'])\n",
    "train['minute'] = train['datetime'].dt.minute\n",
    "train['minute'] = train['minute'] // 10 * 10\n",
    "train['hour'] = train['datetime'].dt.hour\n",
    "train['weekday'] = train['datetime'].dt.weekday\n",
    "train['month'] = train['datetime'].dt.month\n",
    "train['day'] = train['datetime'].dt.day\n",
    "train['year'] = train['datetime'].dt.year\n",
    "train.drop(['datetime'], axis=1, inplace=True)\n",
    "\n",
    "test['datetime'] = pd.to_datetime(test['datetime'])\n",
    "test['minute'] = test['datetime'].dt.minute\n",
    "test['minute'] = test['minute'] // 10 * 10\n",
    "test['hour'] = test['datetime'].dt.hour\n",
    "test['weekday'] = test['datetime'].dt.weekday\n",
    "test['month'] = test['datetime'].dt.month\n",
    "test['day'] = test['datetime'].dt.day\n",
    "test['year'] = test['datetime'].dt.year\n",
    "test.drop(['datetime'], axis=1, inplace=True)\n",
    "\n",
    "\n",
    "API = \"gGryFchORUuq8hXITjVLWQ\"\n",
    "def get_weather(year, month, day, hour, minute):\n",
    "    url = f\"https://apihub.kma.go.kr/api/typ01/url/kma_sfctm2.php?tm1={time_str}&stn=133&help=1&authKey={API}\"\n",
    "\n",
    "    response = requests.get(url)\n",
    "    raw_data = [i for i in response.text.split(\"\\n\")[-3].split(' ')if i!='']\n",
    "    WS, TEMP, HUMI, RN = raw_data[3], raw_data[11], raw_data[13], raw_data[15]\n",
    "    return WS, TEMP, HUMI, RN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'weather_data.csv' not in os.listdir():\n",
    "    data = []\n",
    "    for month in range(3,12):\n",
    "        url = f\"https://apihub.kma.go.kr/api/typ01/url/kma_sfctm3.php?tm1=2023{str(month).zfill(2)}010000&tm2=2023{str(month+1).zfill(2)}010000&stn=133&help=0&authKey={API}\"\n",
    "        response = requests.get(url)\n",
    "        raw_data = [[i for i in response.text.split(\"\\n\")[j].split(' ')if i!=''] for j in range(4,len(response.text.split(\"\\n\")))]\n",
    "        data.append(raw_data)\n",
    "\n",
    "    for month in range(1,5):\n",
    "        url = f\"https://apihub.kma.go.kr/api/typ01/url/kma_sfctm3.php?tm1=2024{str(month).zfill(2)}010000&tm2=2024{str(month+1).zfill(2)}010000&stn=133&help=0&authKey={API}\"\n",
    "        response = requests.get(url)\n",
    "        raw_data = [[i for i in response.text.split(\"\\n\")[j].split(' ')if i!=''] for j in range(4,len(response.text.split(\"\\n\")))]\n",
    "        data.append(raw_data)\n",
    "\n",
    "    total_data = []\n",
    "    for i in range(len(data)):\n",
    "        for j in range(len(data[i])-1):\n",
    "            try:\n",
    "                new_data = [data[i][j][0], data[i][j][3], data[i][j][11], data[i][j][13], data[i][j][15]]\n",
    "                total_data.append(new_data)\n",
    "            except:\n",
    "                continue\n",
    "\n",
    "    #make the data into csv\n",
    "    total_data = pd.DataFrame(total_data, columns=['datetime', 'WS', 'TEMP', 'HUMI', 'RN'])\n",
    "    total_data.to_csv('weather_data.csv', index=False)\n",
    "\n",
    "else:\n",
    "    total_data = pd.read_csv('weather_data.csv', encoding='utf-8-sig', engine='python')\n",
    "\n",
    "# change ['datetime'] to datetime type with format 'YYYYMMDDHHMM'\n",
    "total_data['datetime'] = pd.to_datetime(total_data['datetime'], format='%Y%m%d%H%M')\n",
    "total_data['hour'] = total_data['datetime'].dt.hour\n",
    "total_data['month'] = total_data['datetime'].dt.month\n",
    "total_data['day'] = total_data['datetime'].dt.day\n",
    "total_data['year'] = total_data['datetime'].dt.year\n",
    "total_data['holiday'] = total_data['datetime'].apply(lambda x: 1 if x in holidays.KR() else 0)\n",
    "total_data.drop(['datetime'], axis=1, inplace=True)\n",
    "train = pd.merge(train, total_data, on=['year', 'month', 'day', 'hour'], how='left')\n",
    "test = pd.merge(test, total_data, on=['year', 'month', 'day', 'hour'], how='left')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_num = 50\n",
    "if 'kmeans_model.pkl' not in os.listdir():\n",
    "    # cluster by x_axis and y_axis\n",
    "    kmeans = KMeans(n_clusters=cluster_num, random_state=5).fit(train[['x_axis', 'y_axis']])\n",
    "    train['cluster'] = kmeans.predict(train[['x_axis', 'y_axis']])\n",
    "    test['cluster'] = kmeans.predict(test[['x_axis', 'y_axis']])\n",
    "\n",
    "    # save the kmeans model\n",
    "    import pickle\n",
    "    with open('kmeans_model.pkl', 'wb') as f:\n",
    "        pickle.dump(kmeans, f)\n",
    "\n",
    "else:\n",
    "    with open('kmeans_model.pkl', 'rb') as f:\n",
    "        kmeans = pickle.load(f)\n",
    "    train['cluster'] = kmeans.predict(train[['x_axis', 'y_axis']])\n",
    "    test['cluster'] = kmeans.predict(test[['x_axis', 'y_axis']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "ktv = pd.read_csv('/home/sb/taxi/data/Drinks/dj_ktv.csv', encoding='utf-8-sig', engine='python')\n",
    "karaoke = pd.read_csv('/home/sb/taxi/data/Drinks/dj_karaoke.csv', encoding='utf-8-sig', engine='python')\n",
    "hospital = pd.read_csv('/home/sb/taxi/data/Hospitals/dj_hospital.csv', encoding='utf-8-sig', engine='python')\n",
    "small_hospital = pd.read_csv('/home/sb/taxi/data/Hospitals/dj_small_hospital.csv', encoding='utf-8-sig', engine='python')\n",
    "camping = pd.read_csv('/home/sb/taxi/data/Hotels/dj_camping.csv', encoding='utf-8-sig', engine='python')\n",
    "country_hotel = pd.read_csv('/home/sb/taxi/data/Hotels/dj_country_hotel.csv', encoding='utf-8-sig', engine='python')\n",
    "foreign_hotel = pd.read_csv('/home/sb/taxi/data/Hotels/dj_foreign_hotel.csv', encoding='utf-8-sig', engine='python')\n",
    "hotel = pd.read_csv('/home/sb/taxi/data/Hotels/dj_hotel.csv', encoding='utf-8-sig', engine='python')\n",
    "tour_hotel = pd.read_csv('/home/sb/taxi/data/Hotels/dj_tour_hotel.csv', encoding='utf-8-sig', engine='python')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rename 경도 to x_axis, 위도 to y_axis\n",
    "ktv.rename(columns={'경도':'x_axis', '위도':'y_axis'}, inplace=True)\n",
    "karaoke.rename(columns={'경도':'x_axis', '위도':'y_axis'}, inplace=True)\n",
    "hospital.rename(columns={'경도':'x_axis', '위도':'y_axis'}, inplace=True)\n",
    "small_hospital.rename(columns={'경도':'x_axis', '위도':'y_axis'}, inplace=True)\n",
    "camping.rename(columns={'경도':'x_axis', '위도':'y_axis'}, inplace=True)\n",
    "country_hotel.rename(columns={'경도':'x_axis', '위도':'y_axis'}, inplace=True)\n",
    "foreign_hotel.rename(columns={'경도':'x_axis', '위도':'y_axis'}, inplace=True)\n",
    "hotel.rename(columns={'경도':'x_axis', '위도':'y_axis'}, inplace=True)\n",
    "tour_hotel.rename(columns={'경도':'x_axis', '위도':'y_axis'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "ktv['cluster'] = kmeans.predict(ktv[['x_axis', 'y_axis']])\n",
    "karaoke['cluster'] = kmeans.predict(karaoke[['x_axis', 'y_axis']])\n",
    "hospital['cluster'] = kmeans.predict(hospital[['x_axis', 'y_axis']])\n",
    "small_hospital['cluster'] = kmeans.predict(small_hospital[['x_axis', 'y_axis']])\n",
    "camping['cluster'] = kmeans.predict(camping[['x_axis', 'y_axis']])\n",
    "country_hotel['cluster'] = kmeans.predict(country_hotel[['x_axis', 'y_axis']])\n",
    "foreign_hotel['cluster'] = kmeans.predict(foreign_hotel[['x_axis', 'y_axis']])\n",
    "hotel['cluster'] = kmeans.predict(hotel[['x_axis', 'y_axis']])\n",
    "tour_hotel['cluster'] = kmeans.predict(tour_hotel[['x_axis', 'y_axis']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "hospital['업태구분명'] = pd.Categorical(hospital['업태구분명']).codes\n",
    "number_of_hospital_type = len(hospital['업태구분명'].unique())\n",
    "\n",
    "small_hospital['업태구분명'] = pd.Categorical(small_hospital['업태구분명']).codes\n",
    "number_of_small_hospital_type = len(small_hospital['업태구분명'].unique())\n",
    "\n",
    "new_columns = ['sum_drinks', 'sum_hospitals', 'sum_hotels', 'sum_drinks_area', \n",
    "               'sum_hospital_rooms'] + [f'sum_hospital_type_{i}' for i in range(number_of_hospital_type)] + [f'sum_small_hospital_type_{i}' for i in range(number_of_small_hospital_type)]\n",
    "train[new_columns] = 0\n",
    "\n",
    "drink_dfs = [ktv, karaoke]\n",
    "hospital_dfs = [hospital, small_hospital]\n",
    "hotel_dfs = [hotel]\n",
    "\n",
    "\n",
    "def sum_for_cluster(df, cluster, column=None):\n",
    "    mask = df['cluster'] == cluster\n",
    "    return len(df[mask]) if column is None else df.loc[mask, column].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_265445/1450656950.py:7: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise an error in a future version of pandas. Value '2116.95' has dtype incompatible with int64, please explicitly cast to a compatible dtype first.\n",
      "  train.loc[cluster_mask, 'sum_drinks_area'] = sum(sum_for_cluster(df, i, '시설총규모') for df in drink_dfs)\n"
     ]
    }
   ],
   "source": [
    "for i in range(cluster_num):\n",
    "    cluster_mask = train['cluster'] == i\n",
    "    test_cluster_mask = test['cluster'] == i\n",
    "    \n",
    "    # Sum drinks and drink areas\n",
    "    train.loc[cluster_mask, 'sum_drinks'] = sum(sum_for_cluster(df, i) for df in drink_dfs)\n",
    "    train.loc[cluster_mask, 'sum_drinks_area'] = sum(sum_for_cluster(df, i, '시설총규모') for df in drink_dfs)\n",
    "    \n",
    "    # Sum hospitals and hospital rooms\n",
    "    train.loc[cluster_mask, 'sum_hospitals'] = sum(sum_for_cluster(df, i) for df in hospital_dfs)\n",
    "    train.loc[cluster_mask, 'sum_hospital_rooms'] = sum(sum_for_cluster(df, i, '병상수') for df in hospital_dfs)\n",
    "    \n",
    "    # Sum hospital types\n",
    "    for j in range(number_of_hospital_type):\n",
    "        train.loc[cluster_mask, f'sum_hospital_type_{j}'] = sum(\n",
    "            sum_for_cluster(df[df['업태구분명'] == j], i) for df in hospital_dfs\n",
    "        )\n",
    "    \n",
    "    for k in range(number_of_small_hospital_type):\n",
    "        train.loc[cluster_mask, f'sum_small_hospital_type_{k}'] = sum(\n",
    "            sum_for_cluster(df[df['업태구분명'] == k], i) for df in hospital_dfs\n",
    "        )\n",
    "    \n",
    "    # Sum hotels\n",
    "    train.loc[cluster_mask, 'sum_hotels'] = sum(sum_for_cluster(df, i) for df in hotel_dfs)\n",
    "\n",
    "    # Test data\n",
    "    test.loc[test_cluster_mask, 'sum_drinks'] = sum(sum_for_cluster(df, i) for df in drink_dfs)\n",
    "    test.loc[test_cluster_mask, 'sum_drinks_area'] = sum(sum_for_cluster(df, i, '시설총규모') for df in drink_dfs)\n",
    "\n",
    "    test.loc[test_cluster_mask, 'sum_hospitals'] = sum(sum_for_cluster(df, i) for df in hospital_dfs)\n",
    "    test.loc[test_cluster_mask, 'sum_hospital_rooms'] = sum(sum_for_cluster(df, i, '병상수') for df in hospital_dfs)\n",
    "\n",
    "    for j in range(number_of_hospital_type):\n",
    "        test.loc[test_cluster_mask, f'sum_hospital_type_{j}'] = sum(\n",
    "            sum_for_cluster(df[df['업태구분명'] == j], i) for df in hospital_dfs\n",
    "        )\n",
    "\n",
    "    for k in range(number_of_small_hospital_type):\n",
    "        test.loc[test_cluster_mask, f'sum_small_hospital_type_{k}'] = sum(\n",
    "            sum_for_cluster(df[df['업태구분명'] == k], i) for df in hospital_dfs\n",
    "        )\n",
    "\n",
    "    test.loc[test_cluster_mask, 'sum_hotels'] = sum(sum_for_cluster(df, i) for df in hotel_dfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 동시간대에 cluster 별로 trip의 수를 count\n",
    "train['count'] = 1\n",
    "train_count = train.groupby(['cluster', 'minute', 'hour', 'day', 'month', 'year']).count().reset_index()\n",
    "train_count = train_count[['cluster', 'minute', 'hour', 'day', 'month', 'year','count']]\n",
    "\n",
    "train = pd.merge(train, train_count, on=['cluster', 'minute', 'hour', 'day', 'month', 'year'], how='left')\n",
    "train['count'] = train['count_y']\n",
    "train.drop(['count_x', 'count_y'], axis=1, inplace=True)\n",
    "\n",
    "test['count'] = 1\n",
    "test_count = test.groupby(['cluster', 'minute', 'hour', 'day', 'month', 'year']).count().reset_index()\n",
    "test_count = test_count[['cluster', 'minute', 'hour', 'day', 'month', 'year','count']]\n",
    "\n",
    "test = pd.merge(test, test_count, on=['cluster', 'minute', 'hour', 'day', 'month', 'year'], how='left')\n",
    "test['count'] = test['count_y']\n",
    "test.drop(['count_x', 'count_y'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save the data\n",
    "train.to_csv('train_taxi_50.csv', index=False)\n",
    "test.to_csv('test_taxi_50.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# xgb regression\n",
    "model = xgb.XGBRegressor(n_estimators=1000, max_depth=10, learning_rate=0.05, random_state=5)\n",
    "train_columns = ['hour', 'weekday', 'month', 'day', 'WS', 'TEMP', 'HUMI', 'RN', 'cluster', 'holiday'] + new_columns\n",
    "\n",
    "model.fit(train[train_columns], train['count'])\n",
    "#save the model\n",
    "\n",
    "with open('xgb_model.pkl', 'wb') as f:\n",
    "    pickle.dump(model, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7694412663529593\n"
     ]
    }
   ],
   "source": [
    "# test accuracy\n",
    "pred = model.predict(test[train_columns])\n",
    "print(np.mean(np.abs(pred - test['count'])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# shap\n",
    "explainer = shap.Explainer(model, test[train_columns])\n",
    "#save the explainer\n",
    "with open('explainer.pkl', 'wb') as f:\n",
    "    pickle.dump(explainer, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Map has been saved as 'map_with_markers.html'\n"
     ]
    }
   ],
   "source": [
    "import folium\n",
    "import pandas as pd\n",
    "\n",
    "# cluster centers\n",
    "data = {\n",
    "    'name': ['cluster_' + str(i) for i in range(cluster_num)],\n",
    "    'latitude': [i[1] for i in kmeans.cluster_centers_],\n",
    "    'longitude': [i[0] for i in kmeans.cluster_centers_]\n",
    "}\n",
    "\n",
    "# Create a DataFrame\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Calculate the center of the map\n",
    "center_lat = df['latitude'].mean()\n",
    "center_lon = df['longitude'].mean()\n",
    "\n",
    "# Create a map\n",
    "m = folium.Map(location=[center_lat, center_lon], zoom_start=12)\n",
    "\n",
    "# Add markers to the map\n",
    "for idx, row in df.iterrows():\n",
    "    folium.Marker(\n",
    "        location=[row['latitude'], row['longitude']],\n",
    "        popup=row['name'],\n",
    "        tooltip=row['name']\n",
    "    ).add_to(m)\n",
    "\n",
    "# Save the map\n",
    "m.save(\"map_with_markers.html\")\n",
    "\n",
    "print(\"Map has been saved as 'map_with_markers.html'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "minute = None\n",
    "# load the kmeans model\n",
    "with open('kmeans_model.pkl', 'rb') as f:\n",
    "    kmeans = pickle.load(f)\n",
    "\n",
    "# load the xgb model\n",
    "with open('xgb_model.pkl', 'rb') as f:\n",
    "    model = pickle.load(f)\n",
    "\n",
    "# load the explainer\n",
    "with open('explainer.pkl', 'rb') as f:\n",
    "    explainer = pickle.load(f)\n",
    "\n",
    "cluster_centers = kmeans.cluster_centers_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[127.32078586  36.37711171]\n",
      " [127.40598292  36.32439321]\n",
      " [127.38271699  36.36960215]\n",
      " [127.45697618  36.30374648]\n",
      " [127.42511807  36.44750205]\n",
      " [127.33876665  36.30440491]\n",
      " [127.4309554   36.35705376]\n",
      " [127.34283361  36.35455921]\n",
      " [127.37912166  36.31428959]\n",
      " [127.39446659  36.41901964]\n",
      " [127.42962293  36.32812206]\n",
      " [127.39042386  36.34477034]\n",
      " [127.25611147  36.50098314]\n",
      " [127.37319833  36.35085593]\n",
      " [127.44949836  36.34035747]]\n"
     ]
    }
   ],
   "source": [
    "print(cluster_centers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cluster_0 : 1.5999970436096191\n",
      "cluster_1 : 3.3072245121002197\n",
      "cluster_2 : 2.6796555519104004\n",
      "cluster_3 : 2.268153429031372\n",
      "cluster_4 : 0.9975440502166748\n",
      "cluster_5 : 1.8796712160110474\n",
      "cluster_6 : 4.565709114074707\n",
      "cluster_7 : 3.08294415473938\n",
      "cluster_8 : 2.359192371368408\n",
      "cluster_9 : 1.9838122129440308\n",
      "cluster_10 : 6.028375148773193\n",
      "cluster_11 : 4.285617351531982\n",
      "cluster_12 : 0.6500904560089111\n",
      "cluster_13 : 3.5307488441467285\n",
      "cluster_14 : 1.8716413974761963\n"
     ]
    }
   ],
   "source": [
    "while True:\n",
    "    time.sleep(1)\n",
    "    temp_time = time.strftime('%Y%m%d%H%M', time.localtime(time.time()))\n",
    "    if minute == int(temp_time[10:12]):\n",
    "        continue\n",
    "    else:\n",
    "        year, month, day, hour, minute = int(temp_time[:4]), int(temp_time[4:6]), int(temp_time[6:8]), int(temp_time[8:10]), int(temp_time[10:12])\n",
    "        weekday = pd.to_datetime(f'{year}-{month}-{day}').weekday()\n",
    "        minute = minute // 10 * 10\n",
    "        WS, TEMP, HUMI, RN = get_weather(year, month, day, hour, minute)\n",
    "        holiday = 1 if f'{year}{str(month).zfill(2)}{day}' in holidays.KR() else 0\n",
    "        for i in range(cluster_num):\n",
    "            x_axis, y_axis = cluster_centers[i]\n",
    "            # kmeans.predict([[x_axis, y_axis]])\n",
    "            new_data = pd.DataFrame([[x_axis, y_axis, minute, hour, day, weekday, month, year, WS, TEMP, HUMI, RN, i, holiday]], columns=['x_axis', 'y_axis', 'minute', 'hour', 'day', 'weekday', 'month', 'year', 'WS', 'TEMP', 'HUMI', 'RN', 'cluster', 'holiday'])\n",
    "            new_data['WS'] = new_data['WS'].astype(float)\n",
    "            new_data['TEMP'] = new_data['TEMP'].astype(float)\n",
    "            new_data['HUMI'] = new_data['HUMI'].astype(float)\n",
    "            new_data['RN'] = new_data['RN'].astype(float)\n",
    "            pred = model.predict(new_data[train_columns])\n",
    "            shap_values = explainer(new_data[train_columns])\n",
    "            # shap.plots.waterfall(shap_values[0])\n",
    "            print(f'cluster_{i} : {pred[0]}')\n",
    "\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "welec",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
