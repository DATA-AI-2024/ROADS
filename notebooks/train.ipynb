{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import configparser\n",
    "import os\n",
    "import pickle\n",
    "import warnings\n",
    "from typing import List, Tuple, Dict\n",
    "import json\n",
    "\n",
    "import holidays\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import requests\n",
    "import shap\n",
    "import xgboost as xgb\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.model_selection import train_test_split\n",
    "import optuna\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Read configuration\n",
    "config = configparser.ConfigParser()\n",
    "config.read(\"../scripts/config_train.ini\")\n",
    "\n",
    "# Get configuration values\n",
    "SEED = int(config[\"GENERAL\"][\"seed\"])\n",
    "CLUSTER_NUM = int(config[\"TRAIN\"][\"cluster_num\"])\n",
    "HOUR_INTERVAL = int(config[\"TRAIN\"][\"hour_interval\"])\n",
    "FLOAT_COLUMNS = eval(config[\"TRAIN\"][\"float_columns\"])\n",
    "CATEGORICAL_COLUMNS = eval(config[\"TRAIN\"][\"categorical_columns\"])\n",
    "TARGET_COLUMN = eval(config[\"TRAIN\"][\"target_column\"])\n",
    "TRAIN_COLUMNS = FLOAT_COLUMNS + CATEGORICAL_COLUMNS\n",
    "TRAIN_FILE = config[\"TRAIN\"][\"train_file\"]\n",
    "TEST_FILE = config[\"TRAIN\"][\"test_file\"]\n",
    "WEATHER_FILE = config[\"TRAIN\"][\"weather_file\"]\n",
    "WEATHER_API = config[\"TRAIN\"][\"weather_api\"]\n",
    "SAVE_PATH = config[\"TRAIN\"][\"save_path\"]\n",
    "CLUSTERTING_MODEL_PATH = config[\"TRAIN\"][\"clustering_model_path\"]\n",
    "CLUSTER_FEATURES_PATH = config[\"TRAIN\"][\"cluster_features_path\"]\n",
    "REMAINING_CLUSTERS_PATH = config[\"TRAIN\"][\"remaining_clusters_path\"]\n",
    "XGB_MODEL_PATH = config[\"TRAIN\"][\"xgb_model_path\"]\n",
    "EXPLAINER_PATH = config[\"TRAIN\"][\"explainer_path\"]\n",
    "DATA_PATH = config[\"TRAIN\"][\"data_path\"]\n",
    "\n",
    "# Set random seed\n",
    "np.random.seed(SEED)\n",
    "random_state = SEED"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(file_path: str) -> pd.DataFrame:\n",
    "    \"\"\"Load and preprocess data from CSV file.\"\"\"\n",
    "    df = pd.read_csv(file_path, encoding=\"utf-8-sig\", engine=\"python\")\n",
    "    df = df[(df[\"x_axis\"] != 0) & (df[\"y_axis\"] != 0)]\n",
    "    df[\"datetime\"] = pd.to_datetime(df[\"datetime\"])\n",
    "    df[\"minute\"] = df[\"datetime\"].dt.minute // HOUR_INTERVAL\n",
    "    df[\"hour\"] = df[\"datetime\"].dt.hour\n",
    "    df[\"weekday\"] = df[\"datetime\"].dt.weekday\n",
    "    df[\"month\"] = df[\"datetime\"].dt.month\n",
    "    df[\"day\"] = df[\"datetime\"].dt.day\n",
    "    df[\"year\"] = df[\"datetime\"].dt.year\n",
    "    df.drop([\"datetime\"], axis=1, inplace=True)\n",
    "    return df\n",
    "\n",
    "\n",
    "def get_weather_data() -> pd.DataFrame:\n",
    "    \"\"\"Fetch or load weather data.\"\"\"\n",
    "    \n",
    "    if WEATHER_FILE == \"None\":\n",
    "        data = []\n",
    "        for month in range(3, 12):\n",
    "            url = f\"https://apihub.kma.go.kr/api/typ01/url/kma_sfctm3.php?tm1=2023{str(month).zfill(2)}010000&tm2=2023{str(month+1).zfill(2)}010000&stn=133&help=0&authKey={WEATHER_API}\"\n",
    "            response = requests.get(url)\n",
    "            raw_data = [\n",
    "                [i for i in response.text.split(\"\\n\")[j].split(\" \") if i != \"\"]\n",
    "                for j in range(4, len(response.text.split(\"\\n\")))\n",
    "            ]\n",
    "            data.extend(raw_data)\n",
    "\n",
    "        for month in range(1, 5):\n",
    "            url = f\"https://apihub.kma.go.kr/api/typ01/url/kma_sfctm3.php?tm1=2024{str(month).zfill(2)}010000&tm2=2024{str(month+1).zfill(2)}010000&stn=133&help=0&authKey={WEATHER_API}\"\n",
    "            response = requests.get(url)\n",
    "            raw_data = [\n",
    "                [i for i in response.text.split(\"\\n\")[j].split(\" \") if i != \"\"]\n",
    "                for j in range(4, len(response.text.split(\"\\n\")))\n",
    "            ]\n",
    "            data.extend(raw_data)\n",
    "\n",
    "        total_data = []\n",
    "        for row in data:\n",
    "            try:\n",
    "                new_data = [row[0], row[3], row[11], row[13], row[15]]\n",
    "                total_data.append(new_data)\n",
    "            except:\n",
    "                continue\n",
    "\n",
    "        weather_df = pd.DataFrame(\n",
    "            total_data, columns=[\"datetime\", \"WS\", \"TEMP\", \"HUMI\", \"RN\"]\n",
    "        )\n",
    "        weather_df.to_csv(\"data/weather.csv\", index=False)\n",
    "    else:\n",
    "        weather_df = pd.read_csv(WEATHER_FILE, encoding=\"utf-8-sig\", engine=\"python\")\n",
    "\n",
    "    weather_df[\"datetime\"] = pd.to_datetime(weather_df[\"datetime\"], format=\"%Y%m%d%H%M\")\n",
    "    weather_df[\"hour\"] = weather_df[\"datetime\"].dt.hour\n",
    "    weather_df[\"month\"] = weather_df[\"datetime\"].dt.month\n",
    "    weather_df[\"day\"] = weather_df[\"datetime\"].dt.day\n",
    "    weather_df[\"year\"] = weather_df[\"datetime\"].dt.year\n",
    "    weather_df[\"holiday\"] = weather_df[\"datetime\"].apply(\n",
    "        lambda x: 1 if x in holidays.KR() else 0\n",
    "    )\n",
    "    weather_df.drop([\"datetime\"], axis=1, inplace=True)\n",
    "\n",
    "    return weather_df\n",
    "\n",
    "\n",
    "def perform_clustering(\n",
    "    train_data: pd.DataFrame, test_data: pd.DataFrame\n",
    ") -> Tuple[KMeans, pd.DataFrame, pd.DataFrame]:\n",
    "    \"\"\"Perform KMeans clustering on the data.\"\"\"\n",
    "\n",
    "    kmeans = KMeans(n_clusters=CLUSTER_NUM, random_state=random_state).fit(\n",
    "            train_data[[\"x_axis\", \"y_axis\"]]\n",
    "        )\n",
    "\n",
    "    train_data[\"cluster\"] = kmeans.predict(train_data[[\"x_axis\", \"y_axis\"]])\n",
    "    test_data[\"cluster\"] = kmeans.predict(test_data[[\"x_axis\", \"y_axis\"]])\n",
    "\n",
    "    # Save clustering model\n",
    "    with open(f\"{SAVE_PATH}/{CLUSTERTING_MODEL_PATH}\", \"wb\") as f:\n",
    "        pickle.dump(kmeans, f)\n",
    "\n",
    "    return kmeans, train_data, test_data\n",
    "\n",
    "\n",
    "def filter_clusters(\n",
    "    train_data: pd.DataFrame, test_data: pd.DataFrame\n",
    ") -> Tuple[pd.DataFrame, pd.DataFrame, List[int]]:\n",
    "    \"\"\"Filter out clusters with less than 900 data points.\"\"\"\n",
    "    cluster = train_data.groupby(\"cluster\").size().reset_index()\n",
    "    cluster.columns = [\"cluster\", \"count\"]\n",
    "    excluded_cluster = cluster[cluster[\"count\"] < 900][\"cluster\"].tolist()\n",
    "    remaining_cluster = cluster[cluster[\"count\"] >= 900][\"cluster\"].tolist()\n",
    "\n",
    "    train_data = train_data[train_data[\"cluster\"].isin(remaining_cluster)]\n",
    "    test_data = test_data[test_data[\"cluster\"].isin(remaining_cluster)]\n",
    "\n",
    "    return train_data, test_data, remaining_cluster\n",
    "\n",
    "\n",
    "def add_count_feature(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Add count feature to the dataframe.\"\"\"\n",
    "    df[\"count\"] = 1\n",
    "    count_df = (\n",
    "        df.groupby([\"cluster\", \"hour\", \"day\", \"month\", \"year\"]).count().reset_index()\n",
    "    )\n",
    "    count_df = count_df[[\"cluster\", \"hour\", \"day\", \"month\", \"year\", \"count\"]]\n",
    "\n",
    "    df = pd.merge(\n",
    "        df, count_df, on=[\"cluster\", \"hour\", \"day\", \"month\", \"year\"], how=\"left\"\n",
    "    )\n",
    "    df[\"count\"] = df[\"count_y\"]\n",
    "    df.drop([\"count_x\", \"count_y\"], axis=1, inplace=True)\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "def load_additional_data() -> Dict[str, pd.DataFrame]:\n",
    "    \"\"\"Load additional data sources.\"\"\"\n",
    "    data_sources = {\n",
    "        \"ktv\": \"dj_ktv.csv\",\n",
    "        \"karaoke\": \"dj_karaoke.csv\",\n",
    "        \"hospital\": \"dj_hospital.csv\",\n",
    "        \"small_hospital\": \"dj_small_hospital.csv\",\n",
    "        \"hotel\": \"dj_hotel.csv\",\n",
    "    }\n",
    "\n",
    "    additional_data = {}\n",
    "    for key, filename in data_sources.items():\n",
    "        df = pd.read_csv(\n",
    "            os.path.join(DATA_PATH, filename), encoding=\"utf-8-sig\", engine=\"python\"\n",
    "        )\n",
    "        df.rename(columns={\"경도\": \"x_axis\", \"위도\": \"y_axis\"}, inplace=True)\n",
    "        additional_data[key] = df\n",
    "\n",
    "    return additional_data\n",
    "\n",
    "\n",
    "def add_new_columns(\n",
    "    train_data: pd.DataFrame,\n",
    "    test_data: pd.DataFrame,\n",
    "    kmeans: KMeans,\n",
    "    additional_data: Dict[str, pd.DataFrame],\n",
    ") -> Tuple[pd.DataFrame, pd.DataFrame]:\n",
    "    \"\"\"Add new columns based on additional data sources.\"\"\"\n",
    "    for df in additional_data.values():\n",
    "        df[\"cluster\"] = kmeans.predict(df[[\"x_axis\", \"y_axis\"]])\n",
    "\n",
    "    hospital = additional_data[\"hospital\"]\n",
    "    small_hospital = additional_data[\"small_hospital\"]\n",
    "\n",
    "    hospital[\"업태구분명\"] = pd.Categorical(hospital[\"업태구분명\"]).codes\n",
    "    number_of_hospital_type = len(hospital[\"업태구분명\"].unique())\n",
    "\n",
    "    small_hospital[\"업태구분명\"] = pd.Categorical(small_hospital[\"업태구분명\"]).codes\n",
    "    number_of_small_hospital_type = len(small_hospital[\"업태구분명\"].unique())\n",
    "\n",
    "    new_columns = (\n",
    "        [\n",
    "            \"sum_drinks\",\n",
    "            \"sum_hospitals\",\n",
    "            \"sum_hotels\",\n",
    "            \"sum_drinks_area\",\n",
    "            \"sum_hospital_rooms\",\n",
    "        ]\n",
    "        + [f\"sum_hospital_type_{i}\" for i in range(number_of_hospital_type)]\n",
    "        + [f\"sum_small_hospital_type_{i}\" for i in range(number_of_small_hospital_type)]\n",
    "    )\n",
    "\n",
    "    for df in [train_data, test_data]:\n",
    "        df[new_columns] = 0\n",
    "\n",
    "    drink_dfs = [additional_data[\"ktv\"], additional_data[\"karaoke\"]]\n",
    "    hospital_dfs = [hospital, small_hospital]\n",
    "    hotel_dfs = [additional_data[\"hotel\"]]\n",
    "\n",
    "    def sum_for_cluster(df, cluster, column=None):\n",
    "        mask = df[\"cluster\"] == cluster\n",
    "        return len(df[mask]) if column is None else df.loc[mask, column].sum()\n",
    "\n",
    "    for i in range(CLUSTER_NUM):\n",
    "        for df in [train_data, test_data]:\n",
    "            cluster_mask = df[\"cluster\"] == i\n",
    "\n",
    "            df.loc[cluster_mask, \"sum_drinks\"] = sum(\n",
    "                sum_for_cluster(df, i) for df in drink_dfs\n",
    "            )\n",
    "            df.loc[cluster_mask, \"sum_drinks_area\"] = sum(\n",
    "                sum_for_cluster(df, i, \"시설총규모\") for df in drink_dfs\n",
    "            )\n",
    "\n",
    "            df.loc[cluster_mask, \"sum_hospitals\"] = sum(\n",
    "                sum_for_cluster(df, i) for df in hospital_dfs\n",
    "            )\n",
    "            df.loc[cluster_mask, \"sum_hospital_rooms\"] = sum(\n",
    "                sum_for_cluster(df, i, \"병상수\") for df in hospital_dfs\n",
    "            )\n",
    "\n",
    "            for j in range(number_of_hospital_type):\n",
    "                df.loc[cluster_mask, f\"sum_hospital_type_{j}\"] = sum(\n",
    "                    sum_for_cluster(df[df[\"업태구분명\"] == j], i) for df in hospital_dfs\n",
    "                )\n",
    "\n",
    "            for k in range(number_of_small_hospital_type):\n",
    "                df.loc[cluster_mask, f\"sum_small_hospital_type_{k}\"] = sum(\n",
    "                    sum_for_cluster(df[df[\"업태구분명\"] == k], i) for df in hospital_dfs\n",
    "                )\n",
    "\n",
    "            df.loc[cluster_mask, \"sum_hotels\"] = sum(\n",
    "                sum_for_cluster(df, i) for df in hotel_dfs\n",
    "            )\n",
    "\n",
    "    return train_data, test_data\n",
    "\n",
    "\n",
    "def calculate_and_save_cluster_features(kmeans, additional_data):\n",
    "    cluster_features = {}\n",
    "    \n",
    "    for i in range(CLUSTER_NUM):\n",
    "        cluster_features[i] = {\n",
    "            \"sum_drinks\": 0,\n",
    "            \"sum_hospitals\": 0,\n",
    "            \"sum_hotels\": 0,\n",
    "            \"sum_drinks_area\": 0,\n",
    "            \"sum_hospital_rooms\": 0\n",
    "        }\n",
    "        \n",
    "        drink_dfs = [additional_data[\"ktv\"], additional_data[\"karaoke\"]]\n",
    "        hospital_dfs = [additional_data[\"hospital\"], additional_data[\"small_hospital\"]]\n",
    "        hotel_dfs = [additional_data[\"hotel\"]]\n",
    "        \n",
    "        for df in drink_dfs:\n",
    "            df[\"cluster\"] = kmeans.predict(df[[\"x_axis\", \"y_axis\"]])\n",
    "            cluster_mask = df[\"cluster\"] == i\n",
    "            cluster_features[i][\"sum_drinks\"] += len(df[cluster_mask])\n",
    "            cluster_features[i][\"sum_drinks_area\"] += df.loc[cluster_mask, \"시설총규모\"].sum()\n",
    "        \n",
    "        for df in hospital_dfs:\n",
    "            df[\"cluster\"] = kmeans.predict(df[[\"x_axis\", \"y_axis\"]])\n",
    "            cluster_mask = df[\"cluster\"] == i\n",
    "            cluster_features[i][\"sum_hospitals\"] += len(df[cluster_mask])\n",
    "            cluster_features[i][\"sum_hospital_rooms\"] += df.loc[cluster_mask, \"병상수\"].sum()\n",
    "        \n",
    "        for df in hotel_dfs:\n",
    "            df[\"cluster\"] = kmeans.predict(df[[\"x_axis\", \"y_axis\"]])\n",
    "            cluster_mask = df[\"cluster\"] == i\n",
    "            cluster_features[i][\"sum_hotels\"] += len(df[cluster_mask])\n",
    "    \n",
    "    #change all values in cluster_features to int\n",
    "    for i in range(CLUSTER_NUM):\n",
    "        for key in cluster_features[i].keys():\n",
    "            cluster_features[i][key] = int(cluster_features[i][key])\n",
    "    \n",
    "    # Save the cluster features to a JSON file\n",
    "    \n",
    "    with open(f\"{SAVE_PATH}/{CLUSTER_FEATURES_PATH}\", \"w\") as f:\n",
    "        json.dump(cluster_features, f)\n",
    "\n",
    "\n",
    "def objective(trial, X, y):\n",
    "    \"\"\"Optuna objective function for XGBoost optimization.\"\"\"\n",
    "    params = {\n",
    "        \"n_estimators\": trial.suggest_int(\"n_estimators\", 100, 1000),\n",
    "        \"max_depth\": trial.suggest_int(\"max_depth\", 3, 10),\n",
    "        \"learning_rate\": trial.suggest_loguniform(\"learning_rate\", 1e-3, 1.0),\n",
    "        \"subsample\": trial.suggest_uniform(\"subsample\", 0.6, 1.0),\n",
    "        \"colsample_bytree\": trial.suggest_uniform(\"colsample_bytree\", 0.6, 1.0),\n",
    "        \"min_child_weight\": trial.suggest_int(\"min_child_weight\", 1, 10),\n",
    "        \"random_state\": random_state,\n",
    "        \"enable_categorical\": True,\n",
    "    }\n",
    "\n",
    "    model = xgb.XGBRegressor(**params)\n",
    "\n",
    "    X_train, X_val, y_train, y_val = train_test_split(\n",
    "        X, y, test_size=0.1, random_state=random_state\n",
    "    )\n",
    "\n",
    "    model.fit(X_train, y_train)\n",
    "    predictions = model.predict(X_val)\n",
    "    mae = np.mean(np.abs(predictions - y_val.values.ravel()))\n",
    "\n",
    "    return mae\n",
    "\n",
    "\n",
    "def train_model(train_data: pd.DataFrame) -> xgb.XGBRegressor:\n",
    "    \"\"\"Train XGBoost model using Optuna for hyperparameter optimization.\"\"\"\n",
    "    X = train_data[TRAIN_COLUMNS]\n",
    "    y = train_data[\n",
    "        TARGET_COLUMN[0]\n",
    "    ]  # Assuming TARGET_COLUMN is a list with one element\n",
    "\n",
    "    study = optuna.create_study(direction=\"minimize\")\n",
    "    study.optimize(lambda trial: objective(trial, X, y), n_trials=50)\n",
    "\n",
    "    best_params = study.best_params\n",
    "    best_params[\"random_state\"] = random_state\n",
    "    best_params[\"enable_categorical\"] = True\n",
    "\n",
    "    model = xgb.XGBRegressor(**best_params)\n",
    "    model.fit(X, y)\n",
    "\n",
    "    with open(f\"{SAVE_PATH}/{XGB_MODEL_PATH}\", \"wb\") as f:\n",
    "        pickle.dump(model, f)\n",
    "\n",
    "    print(f\"Best hyperparameters: {best_params}\")\n",
    "    print(f\"Best MAE: {study.best_value}\")\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "def create_explainer(model: xgb.XGBRegressor, test_data: pd.DataFrame):\n",
    "    \"\"\"Create and save SHAP explainer.\"\"\"\n",
    "    explainer = shap.TreeExplainer(model, feature_perturbation='tree_path_dependent')\n",
    "\n",
    "    with open(f\"{SAVE_PATH}/{EXPLAINER_PATH}\", \"wb\") as f:\n",
    "        pickle.dump(explainer, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and preprocess data\n",
    "train_data = load_data(TRAIN_FILE)\n",
    "test_data = load_data(TEST_FILE)\n",
    "\n",
    "# Get weather data\n",
    "weather_data = get_weather_data()\n",
    "\n",
    "# Merge weather data\n",
    "train_data = pd.merge(\n",
    "    train_data, weather_data, on=[\"year\", \"month\", \"day\", \"hour\"], how=\"left\"\n",
    ")\n",
    "test_data = pd.merge(\n",
    "    test_data, weather_data, on=[\"year\", \"month\", \"day\", \"hour\"], how=\"left\"\n",
    ")\n",
    "\n",
    "# Perform clustering\n",
    "kmeans, train_data, test_data = perform_clustering(train_data, test_data)\n",
    "print(\"Clustering completed.\")\n",
    "\n",
    "# Filter clusters\n",
    "train_data, test_data, remaining_cluster = filter_clusters(train_data, test_data)\n",
    "\n",
    "# Save remaining clusters\n",
    "with open(f\"{SAVE_PATH}/{REMAINING_CLUSTERS_PATH}\", \"wb\") as f:\n",
    "    pickle.dump(remaining_cluster, f)\n",
    "\n",
    "# Load additional data\n",
    "additional_data = load_additional_data()\n",
    "\n",
    "# Calculate and save cluster features\n",
    "calculate_and_save_cluster_features(kmeans, additional_data)\n",
    "\n",
    "# Add new columns\n",
    "train_data, test_data = add_new_columns(\n",
    "    train_data, test_data, kmeans, additional_data\n",
    ")\n",
    "\n",
    "# Add count feature\n",
    "train_data = add_count_feature(train_data)\n",
    "test_data = add_count_feature(test_data)\n",
    "\n",
    "for col in FLOAT_COLUMNS:\n",
    "    train_data[col] = train_data[col].astype(float)\n",
    "    test_data[col] = test_data[col].astype(float)\n",
    "\n",
    "for col in CATEGORICAL_COLUMNS:\n",
    "    train_data[col] = train_data[col].astype(\"category\")\n",
    "    test_data[col] = test_data[col].astype(\"category\")\n",
    "\n",
    "for col in TARGET_COLUMN:\n",
    "    train_data[col] = train_data[col].astype(float)\n",
    "    test_data[col] = test_data[col].astype(float)\n",
    "\n",
    "# Train model\n",
    "model = train_model(train_data)\n",
    "print(\"XGBoost Model training with Optuna optimization completed.\")\n",
    "\n",
    "# Create explainer\n",
    "create_explainer(model, test_data)\n",
    "\n",
    "# Test accuracy\n",
    "pred = model.predict(test_data[TRAIN_COLUMNS])\n",
    "mae = np.mean(np.abs(pred - test_data[\"count\"]))\n",
    "print(f\"Mean Absolute Error: {mae}\")\n",
    "\n",
    "print(\"Clustering Model, XGB Model and explainer saved.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "taxi",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
