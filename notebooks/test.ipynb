{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sb/miniconda3/envs/taxi/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import shap # 0.46.0\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import requests\n",
    "import time\n",
    "import os\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.cluster import KMeans\n",
    "import xgboost as xgb # 2.1.1\n",
    "import matplotlib.pyplot as plt # 3.9.2\n",
    "import holidays # 0.56\n",
    "from geopy.geocoders import Nominatim\n",
    "import pickle\n",
    "\n",
    "train = pd.read_csv('../train_taxi_tims_U.csv', encoding='utf-8-sig', engine='python')\n",
    "test = pd.read_csv('../test_taxi_tims_U.csv', encoding='utf-8-sig', engine='python')\n",
    "\n",
    "train = train[(train['x_axis'] != 0) & (train['y_axis'] != 0) & (train['to_x_axis'] != 0) & (train['to_y_axis'] != 0) & (train['distance'] != 0)]\n",
    "test = test[(test['x_axis'] != 0) & (test['y_axis'] != 0) & (test['to_x_axis'] != 0) & (test['to_y_axis'] != 0) & (test['distance'] != 0)]\n",
    "\n",
    "train['datetime'] = pd.to_datetime(train['datetime'])\n",
    "train['minute'] = train['datetime'].dt.minute\n",
    "train['minute'] = train['minute'] \n",
    "train['hour'] = train['datetime'].dt.hour\n",
    "train['weekday'] = train['datetime'].dt.weekday\n",
    "train['month'] = train['datetime'].dt.month\n",
    "train['day'] = train['datetime'].dt.day\n",
    "train['year'] = train['datetime'].dt.year\n",
    "train.drop(['datetime'], axis=1, inplace=True)\n",
    "\n",
    "test['datetime'] = pd.to_datetime(test['datetime'])\n",
    "test['minute'] = test['datetime'].dt.minute\n",
    "test['minute'] = test['minute']\n",
    "test['hour'] = test['datetime'].dt.hour\n",
    "test['weekday'] = test['datetime'].dt.weekday\n",
    "test['month'] = test['datetime'].dt.month\n",
    "test['day'] = test['datetime'].dt.day\n",
    "test['year'] = test['datetime'].dt.year\n",
    "test.drop(['datetime'], axis=1, inplace=True)\n",
    "\n",
    "API = \"gGryFchORUuq8hXITjVLWQ\"\n",
    "def get_weather(year, month, day, hour, minute):\n",
    "    url = f\"https://apihub.kma.go.kr/api/typ01/url/kma_sfctm2.php?tm1={time_str}&stn=133&help=1&authKey={API}\"\n",
    "\n",
    "    response = requests.get(url)\n",
    "    raw_data = [i for i in response.text.split(\"\\n\")[-3].split(' ')if i!='']\n",
    "    WS, TEMP, HUMI, RN = raw_data[3], raw_data[11], raw_data[13], raw_data[15]\n",
    "    return WS, TEMP, HUMI, RN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'weather_data.csv' not in os.listdir():\n",
    "    data = []\n",
    "    for month in range(3,12):\n",
    "        url = f\"https://apihub.kma.go.kr/api/typ01/url/kma_sfctm3.php?tm1=2023{str(month).zfill(2)}010000&tm2=2023{str(month+1).zfill(2)}010000&stn=133&help=0&authKey={API}\"\n",
    "        response = requests.get(url)\n",
    "        raw_data = [[i for i in response.text.split(\"\\n\")[j].split(' ')if i!=''] for j in range(4,len(response.text.split(\"\\n\")))]\n",
    "        data.append(raw_data)\n",
    "\n",
    "    for month in range(1,5):\n",
    "        url = f\"https://apihub.kma.go.kr/api/typ01/url/kma_sfctm3.php?tm1=2024{str(month).zfill(2)}010000&tm2=2024{str(month+1).zfill(2)}010000&stn=133&help=0&authKey={API}\"\n",
    "        response = requests.get(url)\n",
    "        raw_data = [[i for i in response.text.split(\"\\n\")[j].split(' ')if i!=''] for j in range(4,len(response.text.split(\"\\n\")))]\n",
    "        data.append(raw_data)\n",
    "\n",
    "    total_data = []\n",
    "    for i in range(len(data)):\n",
    "        for j in range(len(data[i])-1):\n",
    "            try:\n",
    "                new_data = [data[i][j][0], data[i][j][3], data[i][j][11], data[i][j][13], data[i][j][15]]\n",
    "                total_data.append(new_data)\n",
    "            except:\n",
    "                continue\n",
    "\n",
    "    #make the data into csv\n",
    "    total_data = pd.DataFrame(total_data, columns=['datetime', 'WS', 'TEMP', 'HUMI', 'RN'])\n",
    "    total_data.to_csv('weather_data.csv', index=False)\n",
    "\n",
    "else:\n",
    "    total_data = pd.read_csv('weather_data.csv', encoding='utf-8-sig', engine='python')\n",
    "\n",
    "# change ['datetime'] to datetime type with format 'YYYYMMDDHHMM'\n",
    "total_data['datetime'] = pd.to_datetime(total_data['datetime'], format='%Y%m%d%H%M')\n",
    "total_data['hour'] = total_data['datetime'].dt.hour\n",
    "total_data['month'] = total_data['datetime'].dt.month\n",
    "total_data['day'] = total_data['datetime'].dt.day\n",
    "total_data['year'] = total_data['datetime'].dt.year\n",
    "total_data['holiday'] = total_data['datetime'].apply(lambda x: 1 if x in holidays.KR() else 0)\n",
    "total_data.drop(['datetime'], axis=1, inplace=True)\n",
    "\n",
    "train = pd.merge(train, total_data, on=['year', 'month', 'day', 'hour'], how='left')\n",
    "test = pd.merge(test, total_data, on=['year', 'month', 'day', 'hour'], how='left')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_num = 100\n",
    "if 'kmeans_model.pkl' not in os.listdir():\n",
    "    # cluster by x_axis and y_axis\n",
    "    kmeans = KMeans(n_clusters=cluster_num, random_state=5).fit(train[['x_axis', 'y_axis']])\n",
    "    train['cluster'] = kmeans.predict(train[['x_axis', 'y_axis']])\n",
    "    test['cluster'] = kmeans.predict(test[['x_axis', 'y_axis']])\n",
    "\n",
    "    # save the kmeans model\n",
    "    import pickle\n",
    "    with open('kmeans_model.pkl', 'wb') as f:\n",
    "        pickle.dump(kmeans, f)\n",
    "\n",
    "else:\n",
    "    with open('kmeans_model.pkl', 'rb') as f:\n",
    "        kmeans = pickle.load(f)\n",
    "    train['cluster'] = kmeans.predict(train[['x_axis', 'y_axis']])\n",
    "    test['cluster'] = kmeans.predict(test[['x_axis', 'y_axis']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# exclude the cluster that has less than 900 data\n",
    "cluster = train.groupby('cluster').size().reset_index()\n",
    "cluster.columns = ['cluster', 'count']\n",
    "# make the list of excluded cluster\n",
    "excluded_cluster = cluster[cluster['count'] < 900]['cluster'].tolist()\n",
    "remaining_cluster = cluster[cluster['count'] >= 900]['cluster'].tolist()\n",
    "\n",
    "cluster = cluster[cluster['count'] >= 900]\n",
    "train = train[train['cluster'].isin(cluster['cluster'])]\n",
    "test = test[test['cluster'].isin(cluster['cluster'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[13, 15, 18, 27, 37, 41, 45, 47, 48, 50, 57, 60, 66, 70, 71, 85, 90, 97, 98]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "excluded_cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "ktv = pd.read_csv('/home/sb/taxi/data/Drinks/dj_ktv.csv', encoding='utf-8-sig', engine='python')\n",
    "karaoke = pd.read_csv('/home/sb/taxi/data/Drinks/dj_karaoke.csv', encoding='utf-8-sig', engine='python')\n",
    "hospital = pd.read_csv('/home/sb/taxi/data/Hospitals/dj_hospital.csv', encoding='utf-8-sig', engine='python')\n",
    "small_hospital = pd.read_csv('/home/sb/taxi/data/Hospitals/dj_small_hospital.csv', encoding='utf-8-sig', engine='python')\n",
    "camping = pd.read_csv('/home/sb/taxi/data/Hotels/dj_camping.csv', encoding='utf-8-sig', engine='python')\n",
    "country_hotel = pd.read_csv('/home/sb/taxi/data/Hotels/dj_country_hotel.csv', encoding='utf-8-sig', engine='python')\n",
    "foreign_hotel = pd.read_csv('/home/sb/taxi/data/Hotels/dj_foreign_hotel.csv', encoding='utf-8-sig', engine='python')\n",
    "hotel = pd.read_csv('/home/sb/taxi/data/Hotels/dj_hotel.csv', encoding='utf-8-sig', engine='python')\n",
    "tour_hotel = pd.read_csv('/home/sb/taxi/data/Hotels/dj_tour_hotel.csv', encoding='utf-8-sig', engine='python')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rename 경도 to x_axis, 위도 to y_axis\n",
    "ktv.rename(columns={'경도':'x_axis', '위도':'y_axis'}, inplace=True)\n",
    "karaoke.rename(columns={'경도':'x_axis', '위도':'y_axis'}, inplace=True)\n",
    "hospital.rename(columns={'경도':'x_axis', '위도':'y_axis'}, inplace=True)\n",
    "small_hospital.rename(columns={'경도':'x_axis', '위도':'y_axis'}, inplace=True)\n",
    "camping.rename(columns={'경도':'x_axis', '위도':'y_axis'}, inplace=True)\n",
    "country_hotel.rename(columns={'경도':'x_axis', '위도':'y_axis'}, inplace=True)\n",
    "foreign_hotel.rename(columns={'경도':'x_axis', '위도':'y_axis'}, inplace=True)\n",
    "hotel.rename(columns={'경도':'x_axis', '위도':'y_axis'}, inplace=True)\n",
    "tour_hotel.rename(columns={'경도':'x_axis', '위도':'y_axis'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "ktv['cluster'] = kmeans.predict(ktv[['x_axis', 'y_axis']])\n",
    "karaoke['cluster'] = kmeans.predict(karaoke[['x_axis', 'y_axis']])\n",
    "hospital['cluster'] = kmeans.predict(hospital[['x_axis', 'y_axis']])\n",
    "small_hospital['cluster'] = kmeans.predict(small_hospital[['x_axis', 'y_axis']])\n",
    "camping['cluster'] = kmeans.predict(camping[['x_axis', 'y_axis']])\n",
    "country_hotel['cluster'] = kmeans.predict(country_hotel[['x_axis', 'y_axis']])\n",
    "foreign_hotel['cluster'] = kmeans.predict(foreign_hotel[['x_axis', 'y_axis']])\n",
    "hotel['cluster'] = kmeans.predict(hotel[['x_axis', 'y_axis']])\n",
    "tour_hotel['cluster'] = kmeans.predict(tour_hotel[['x_axis', 'y_axis']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "hospital['업태구분명'] = pd.Categorical(hospital['업태구분명']).codes\n",
    "number_of_hospital_type = len(hospital['업태구분명'].unique())\n",
    "\n",
    "small_hospital['업태구분명'] = pd.Categorical(small_hospital['업태구분명']).codes\n",
    "number_of_small_hospital_type = len(small_hospital['업태구분명'].unique())\n",
    "\n",
    "new_columns = ['sum_drinks', 'sum_hospitals', 'sum_hotels', 'sum_drinks_area', \n",
    "               'sum_hospital_rooms'] + [f'sum_hospital_type_{i}' for i in range(number_of_hospital_type)] + [f'sum_small_hospital_type_{i}' for i in range(number_of_small_hospital_type)]\n",
    "train[new_columns] = 0\n",
    "\n",
    "drink_dfs = [ktv, karaoke]\n",
    "hospital_dfs = [hospital, small_hospital]\n",
    "hotel_dfs = [hotel]\n",
    "\n",
    "\n",
    "def sum_for_cluster(df, cluster, column=None):\n",
    "    mask = df['cluster'] == cluster\n",
    "    return len(df[mask]) if column is None else df.loc[mask, column].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_26157/3949256275.py:9: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise an error in a future version of pandas. Value '1507.2200000000003' has dtype incompatible with int64, please explicitly cast to a compatible dtype first.\n",
      "  train.loc[cluster_mask, 'sum_drinks_area'] = sum(sum_for_cluster(df, i, '시설총규모') for df in drink_dfs)\n"
     ]
    }
   ],
   "source": [
    "for i in range(cluster_num):\n",
    "    if i in excluded_cluster:\n",
    "        continue\n",
    "    cluster_mask = train['cluster'] == i\n",
    "    test_cluster_mask = test['cluster'] == i\n",
    "    \n",
    "    # Sum drinks and drink areas\n",
    "    train.loc[cluster_mask, 'sum_drinks'] = sum(sum_for_cluster(df, i) for df in drink_dfs)\n",
    "    train.loc[cluster_mask, 'sum_drinks_area'] = sum(sum_for_cluster(df, i, '시설총규모') for df in drink_dfs)\n",
    "    \n",
    "    # Sum hospitals and hospital rooms\n",
    "    train.loc[cluster_mask, 'sum_hospitals'] = sum(sum_for_cluster(df, i) for df in hospital_dfs)\n",
    "    train.loc[cluster_mask, 'sum_hospital_rooms'] = sum(sum_for_cluster(df, i, '병상수') for df in hospital_dfs)\n",
    "    \n",
    "    # Sum hospital types\n",
    "    for j in range(number_of_hospital_type):\n",
    "        train.loc[cluster_mask, f'sum_hospital_type_{j}'] = sum(\n",
    "            sum_for_cluster(df[df['업태구분명'] == j], i) for df in hospital_dfs\n",
    "        )\n",
    "    \n",
    "    for k in range(number_of_small_hospital_type):\n",
    "        train.loc[cluster_mask, f'sum_small_hospital_type_{k}'] = sum(\n",
    "            sum_for_cluster(df[df['업태구분명'] == k], i) for df in hospital_dfs\n",
    "        )\n",
    "    \n",
    "    # Sum hotels\n",
    "    train.loc[cluster_mask, 'sum_hotels'] = sum(sum_for_cluster(df, i) for df in hotel_dfs)\n",
    "\n",
    "    # Test data\n",
    "    test.loc[test_cluster_mask, 'sum_drinks'] = sum(sum_for_cluster(df, i) for df in drink_dfs)\n",
    "    test.loc[test_cluster_mask, 'sum_drinks_area'] = sum(sum_for_cluster(df, i, '시설총규모') for df in drink_dfs)\n",
    "\n",
    "    test.loc[test_cluster_mask, 'sum_hospitals'] = sum(sum_for_cluster(df, i) for df in hospital_dfs)\n",
    "    test.loc[test_cluster_mask, 'sum_hospital_rooms'] = sum(sum_for_cluster(df, i, '병상수') for df in hospital_dfs)\n",
    "\n",
    "    for j in range(number_of_hospital_type):\n",
    "        test.loc[test_cluster_mask, f'sum_hospital_type_{j}'] = sum(\n",
    "            sum_for_cluster(df[df['업태구분명'] == j], i) for df in hospital_dfs\n",
    "        )\n",
    "\n",
    "    for k in range(number_of_small_hospital_type):\n",
    "        test.loc[test_cluster_mask, f'sum_small_hospital_type_{k}'] = sum(\n",
    "            sum_for_cluster(df[df['업태구분명'] == k], i) for df in hospital_dfs\n",
    "        )\n",
    "\n",
    "    test.loc[test_cluster_mask, 'sum_hotels'] = sum(sum_for_cluster(df, i) for df in hotel_dfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 동시간대에 cluster 별로 trip의 수를 count\n",
    "train['count'] = 1\n",
    "train_count = train.groupby(['cluster', 'hour', 'day', 'month', 'year']).count().reset_index()\n",
    "train_count = train_count[['cluster', 'hour', 'day', 'month', 'year','count']]\n",
    "\n",
    "train = pd.merge(train, train_count, on=['cluster', 'hour', 'day', 'month', 'year'], how='left')\n",
    "train['count'] = train['count_y']\n",
    "train.drop(['count_x', 'count_y'], axis=1, inplace=True)\n",
    "\n",
    "test['count'] = 1\n",
    "test_count = test.groupby(['cluster', 'hour', 'day', 'month', 'year']).count().reset_index()\n",
    "test_count = test_count[['cluster', 'hour', 'day', 'month', 'year','count']]\n",
    "\n",
    "test = pd.merge(test, test_count, on=['cluster', 'hour', 'day', 'month', 'year'], how='left')\n",
    "test['count'] = test['count_y']\n",
    "test.drop(['count_x', 'count_y'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# xgb regression\n",
    "model = xgb.XGBRegressor(n_estimators=1000, max_depth=10, learning_rate=0.05, random_state=5)\n",
    "train_columns = ['hour', 'weekday', 'month', 'day', 'WS', 'TEMP', 'HUMI', 'RN', 'cluster', 'holiday'] + new_columns\n",
    "\n",
    "model.fit(train[train_columns], train['count'])\n",
    "#save the model\n",
    "\n",
    "with open('xgb_model.pkl', 'wb') as f:\n",
    "    pickle.dump(model, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.7349732606222685\n"
     ]
    }
   ],
   "source": [
    "# test accuracy\n",
    "pred = model.predict(test[train_columns])\n",
    "print(np.mean(np.abs(pred - test['count'])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# shap\n",
    "explainer = shap.Explainer(model, test[train_columns])\n",
    "#save the explainer\n",
    "with open('explainer.pkl', 'wb') as f:\n",
    "    pickle.dump(explainer, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'name': ['cluster_0', 'cluster_1', 'cluster_2', 'cluster_3', 'cluster_4', 'cluster_5', 'cluster_6', 'cluster_7', 'cluster_8', 'cluster_9', 'cluster_10', 'cluster_11', 'cluster_12', 'cluster_14', 'cluster_16', 'cluster_17', 'cluster_19', 'cluster_20', 'cluster_21', 'cluster_22', 'cluster_23', 'cluster_24', 'cluster_25', 'cluster_26', 'cluster_28', 'cluster_29', 'cluster_30', 'cluster_31', 'cluster_32', 'cluster_33', 'cluster_34', 'cluster_35', 'cluster_36', 'cluster_38', 'cluster_39', 'cluster_40', 'cluster_42', 'cluster_43', 'cluster_44', 'cluster_46', 'cluster_49', 'cluster_51', 'cluster_52', 'cluster_53', 'cluster_54', 'cluster_55', 'cluster_56', 'cluster_58', 'cluster_59', 'cluster_61', 'cluster_62', 'cluster_63', 'cluster_64', 'cluster_65', 'cluster_67', 'cluster_68', 'cluster_69', 'cluster_72', 'cluster_73', 'cluster_74', 'cluster_75', 'cluster_76', 'cluster_77', 'cluster_78', 'cluster_79', 'cluster_80', 'cluster_81', 'cluster_82', 'cluster_83', 'cluster_84', 'cluster_86', 'cluster_87', 'cluster_88', 'cluster_89', 'cluster_91', 'cluster_92', 'cluster_93', 'cluster_94', 'cluster_95', 'cluster_96', 'cluster_99'], 'latitude': [np.float64(36.37224010773024), np.float64(36.32069397611169), np.float64(36.35995750308333), np.float64(36.445845393557796), np.float64(36.29987678027919), np.float64(36.36504975618953), np.float64(36.35449159026967), np.float64(36.31830978278254), np.float64(36.31375242065058), np.float64(36.33256591482411), np.float64(36.34470161506294), np.float64(36.41831585192262), np.float64(36.33597411116803), np.float64(36.38583937099659), np.float64(36.30958433527709), np.float64(36.39698681623401), np.float64(36.33876733098743), np.float64(36.35106963745898), np.float64(36.37098337782644), np.float64(36.356170879402626), np.float64(36.321763271031536), np.float64(36.3532670928405), np.float64(36.33273779803236), np.float64(36.31342156280529), np.float64(36.32151981164232), np.float64(36.32712841234682), np.float64(36.384407793612304), np.float64(36.35907878539822), np.float64(36.30114664475552), np.float64(36.29765684796291), np.float64(36.35139338661289), np.float64(36.33291479744273), np.float64(36.4479146976047), np.float64(36.39123659142122), np.float64(36.34616340599855), np.float64(36.36011461953789), np.float64(36.43465792368826), np.float64(36.364071061858894), np.float64(36.33027012287872), np.float64(36.30513176579298), np.float64(36.33520496857855), np.float64(36.30613729592555), np.float64(36.344898235211595), np.float64(36.344359314312285), np.float64(36.280055844778325), np.float64(36.299681584788786), np.float64(36.390423937486126), np.float64(36.35549855952263), np.float64(36.36681275981051), np.float64(36.35435796204029), np.float64(36.42442855084619), np.float64(36.353322358512344), np.float64(36.32001386631287), np.float64(36.33678322284397), np.float64(36.340077242473434), np.float64(36.320314929718386), np.float64(36.35744449799141), np.float64(36.37619594411217), np.float64(36.33154220491003), np.float64(36.325660252420874), np.float64(36.404805489223364), np.float64(36.350005791474366), np.float64(36.329061527926086), np.float64(36.37131973509802), np.float64(36.343864692002334), np.float64(36.35006453540772), np.float64(36.350813518860306), np.float64(36.35103201787362), np.float64(36.370162681608264), np.float64(36.355818630246915), np.float64(36.334501875939864), np.float64(36.356408178736395), np.float64(36.340618109771555), np.float64(36.34555592436809), np.float64(36.45000547576243), np.float64(36.32276440582087), np.float64(36.369598412630346), np.float64(36.435510575844205), np.float64(36.30646347924809), np.float64(36.361721007836074), np.float64(36.361418985227175)], 'longitude': [np.float64(127.31985837691661), np.float64(127.4125602615032), np.float64(127.39102797610863), np.float64(127.41020827943824), np.float64(127.33805893605837), np.float64(127.43908285231466), np.float64(127.34166688703874), np.float64(127.45874717433047), np.float64(127.37887900098421), np.float64(127.41701889291006), np.float64(127.3822738428608), np.float64(127.38403932105409), np.float64(127.43635374146115), np.float64(127.37561303049972), np.float64(127.39703935389393), np.float64(127.40330534228104), np.float64(127.38977234408469), np.float64(127.43698475418277), np.float64(127.36193956720686), np.float64(127.41125303986688), np.float64(127.34594364446998), np.float64(127.29960163043242), np.float64(127.45529828165695), np.float64(127.43967955438602), np.float64(127.40346002272996), np.float64(127.4250922162495), np.float64(127.30515071284816), np.float64(127.34384190536367), np.float64(127.32207844134436), np.float64(127.45789158472182), np.float64(127.3687954514791), np.float64(127.33778833528432), np.float64(127.42354562846016), np.float64(127.35042774264328), np.float64(127.44846043508328), np.float64(127.42682496250308), np.float64(127.38629309576652), np.float64(127.37258293712998), np.float64(127.40376632220138), np.float64(127.36743717315633), np.float64(127.3716901407995), np.float64(127.35064248106562), np.float64(127.39500306948511), np.float64(127.42026190132131), np.float64(127.46709339468156), np.float64(127.3814396288768), np.float64(127.3159969322326), np.float64(127.33440378412737), np.float64(127.41343925709158), np.float64(127.38081966150048), np.float64(127.39309260268573), np.float64(127.35675139804245), np.float64(127.39276209137013), np.float64(127.44713751629048), np.float64(127.41015622860913), np.float64(127.43039518226524), np.float64(127.3485614967997), np.float64(127.39341977191705), np.float64(127.43225726219185), np.float64(127.44297080906502), np.float64(127.41989070509628), np.float64(127.42813859073156), np.float64(127.4620337984033), np.float64(127.42693747144845), np.float64(127.34105965827385), np.float64(127.38860971047691), np.float64(127.39846915953075), np.float64(127.37662344084394), np.float64(127.38111937733149), np.float64(127.44919533075308), np.float64(127.38232466343642), np.float64(127.36398970046395), np.float64(127.30785352399936), np.float64(127.37244045800401), np.float64(127.43111901740019), np.float64(127.37349548505625), np.float64(127.33883424772311), np.float64(127.42355478261703), np.float64(127.45480928306159), np.float64(127.35293829832291), np.float64(127.37786408423094)]}\n",
      "Map has been saved as 'map_with_markers.html'\n"
     ]
    }
   ],
   "source": [
    "import folium\n",
    "import pandas as pd\n",
    "\n",
    "# cluster centers\n",
    "data = {\n",
    "    'name': ['cluster_' + str(i) for i in range(cluster_num) if i in remaining_cluster],\n",
    "    'latitude': [kmeans.cluster_centers_[i][1] for i in range(cluster_num) if i in remaining_cluster],\n",
    "    'longitude': [kmeans.cluster_centers_[i][0] for i in range(cluster_num) if i in remaining_cluster]\n",
    "}\n",
    "print(data)\n",
    "# Create a DataFrame\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Calculate the center of the map\n",
    "center_lat = df['latitude'].mean()\n",
    "center_lon = df['longitude'].mean()\n",
    "\n",
    "# Create a map\n",
    "m = folium.Map(location=[center_lat, center_lon], zoom_start=12)\n",
    "\n",
    "# Add markers to the map\n",
    "for idx, row in df.iterrows():\n",
    "    folium.Marker(\n",
    "        location=[row['latitude'], row['longitude']],\n",
    "        popup=row['name'],\n",
    "        tooltip=row['name']\n",
    "    ).add_to(m)\n",
    "\n",
    "# Save the map\n",
    "m.save(\"map_with_markers.html\")\n",
    "\n",
    "print(\"Map has been saved as 'map_with_markers.html'\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "taxi",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
